#!/usr/bin/env python
# coding: utf-8

# %%


# 'mkdir -p ../datasets && cd ../datasets && if [ ! -f "input.txt" ]; then curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -o input.txt; fi && cd ../jupyter')


# %%


# read it in to inspect it
with open('../datasets/input.txt', 'r', encoding='utf-8') as f:
    text = f.read()
print(f'length of dataset in characters: {len(text):,}')


# %%



# %%


print(text[:1000]) # check first 1000 chars


# %%


# here are all the unique characters that occur in this text file
chars = sorted(list(set(text)))
vocab_size = len(chars)


# %%


print(''.join(chars))
print(vocab_size)


# %%


# create a mapping from charactors to integers
stoi = { ch:i for i, ch in enumerate(chars)}
itos = { i:ch for i, ch in enumerate(chars)}
encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers
decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string


# %%


print(encode("hi! there"))
print(decode(encode("hi! there")))


# %%


import torch
data = torch.tensor(encode(text), dtype=torch.long)


# %%


print(data.shape, data.dtype)
print(data[:1000])


# %%


# Let's now split up the data into train and validation sets
n = int(0.9 * len(data))
train_data = data[:n]
val_data = data[n:]


# %%


block_size = 8
train_data[:block_size + 1]


# %%


x = train_data[:block_size]
y = train_data[1:block_size+1]
for t in range(block_size):
    context = x[:t+1]
    target  = y[t]
    print(f"when input is {context} the target: {target}")


# %%


torch.manual_seed(1337)
batch_size = 4  # how many independent sequences will we process in parallel?
block_size = 8  # what is the maximum context length for predictions?


# %%


def get_batch(split):
    # generate a small batch of data of inputs x and targets y
    data = train_data if split == 'train' else val_data
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size+1] for i in ix])
    return x, y


# %%


xb, yb = get_batch('train')


# %%


print('inputs:')
print(xb.shape)
print(xb)
print('targets:')
print(yb.shape)
print(yb)

print('------')

for b in range(batch_size):
    for t in range(block_size):
        context = xb[b, :t+1]
        target = yb[b, t]
        print(f"when input is {context.tolist()} the target: {target}")


# `self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)`ã¯ã€**å„å˜èªžï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã«å¯¾ã™ã‚‹ã€Œæ¬¡ã«ã©ã®å˜èªžãŒæ¥ã‚‹ã‹ã€ã®äºˆæ¸¬ã‚¹ã‚³ã‚¢ï¼ˆãƒ­ã‚¸ãƒƒãƒˆï¼‰ã‚’æ ¼ç´ã™ã‚‹ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ†ãƒ¼ãƒ–ãƒ«**ã‚’ä½œæˆã™ã‚‹å½¹å‰²ã‚’æ‹…ã£ã¦ã„ã¾ã™ã€‚
# 
# ã“ã®Bigramãƒ¢ãƒ‡ãƒ«ã®æ–‡è„ˆã§ã€ã‚‚ã†å°‘ã—å…·ä½“çš„ã«è§£èª¬ã—ã¾ã™ã€‚
# 
# ---
# 
# ### ## `nn.Embedding`ã®åŸºæœ¬
# 
# `nn.Embedding`ã¯ã€PyTorchã«ãŠã„ã¦å˜èªžã®ã‚ˆã†ãªé›¢æ•£çš„ãªIDã‚’ã€å¯†ãªãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾ï¼ˆåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã«å¤‰æ›ã™ã‚‹ãŸã‚ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ã™ã€‚åŸºæœ¬çš„ã«ã¯å·¨å¤§ãªè¡Œåˆ—ã§ã‚ã‚Šã€**å˜èªžã®IDã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã—ã¦ã€å¯¾å¿œã™ã‚‹è¡Œãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠœãå‡ºã™**ã¨ã„ã†å‹•ä½œã‚’ã—ã¾ã™ã€‚
# 
# `nn.Embedding(num_embeddings, embedding_dim)` ã®å¼•æ•°ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚
# * `num_embeddings`: åŸ‹ã‚è¾¼ã‚€å˜èªžã®ç·æ•°ã€‚é€šå¸¸ã¯èªžå½™ã‚µã‚¤ã‚º (`vocab_size`) ã‚’æŒ‡å®šã—ã¾ã™ã€‚
# * `embedding_dim`: å„å˜èªžã‚’è¡¨ç¾ã™ã‚‹ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°ã€‚
# 
# ---
# 
# ### ## ã“ã®ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹`nn.Embedding`ã®ç‰¹åˆ¥ãªå½¹å‰²
# 
# ã“ã®`BigramLanguageModel`ã§ã¯ã€`embedding_dim`ã«`vocab_size`ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚ŒãŒé‡è¦ãªãƒã‚¤ãƒ³ãƒˆã§ã™ã€‚
# 
# `self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)`
# 
# ã“ã‚Œã¯ã€ã‚µã‚¤ã‚ºãŒ `(vocab_size, vocab_size)` ã®è¡Œåˆ—ã‚’ä½œæˆã™ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚
# 
# * **è¡Œ (Rows)**: `vocab_size`å€‹ã®è¡Œã¯ã€èªžå½™ã«å«ã¾ã‚Œã‚‹å„ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾å¿œã—ã¾ã™ã€‚ä¾‹ãˆã°ã€0ç•ªç›®ã®è¡Œã¯IDãŒ0ã®ãƒˆãƒ¼ã‚¯ãƒ³ã€1ç•ªç›®ã®è¡Œã¯IDãŒ1ã®ãƒˆãƒ¼ã‚¯ãƒ³...ã¨ã„ã†å…·åˆã§ã™ã€‚
# * **åˆ— (Columns)**: `vocab_size`å€‹ã®åˆ—ã‚’æŒã¤ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆå„è¡Œï¼‰ãŒã€**æ¬¡ã«ã©ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒæ¥ã‚‹ã‹ã®äºˆæ¸¬ã‚¹ã‚³ã‚¢ï¼ˆãƒ­ã‚¸ãƒƒãƒˆï¼‰**ã‚’è¡¨ã—ã¾ã™ã€‚
# 
# #### ### å‡¦ç†ã®æµã‚Œ
# 
# 1.  **å…¥åŠ›**: `forward`ãƒ¡ã‚½ãƒƒãƒ‰ã«ã€ãƒˆãƒ¼ã‚¯ãƒ³IDã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ `idx` (shape: `(B, T)`ã€Bã¯ãƒãƒƒãƒã‚µã‚¤ã‚º, Tã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·) ãŒå…¥åŠ›ã•ã‚Œã¾ã™ã€‚
# 2.  **ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—**: `self.token_embedding_table(idx)` ãŒå®Ÿè¡Œã•ã‚Œã‚‹ã¨ã€`idx` ã®å„ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¯¾å¿œã™ã‚‹è¡Œãƒ™ã‚¯ãƒˆãƒ«ãŒãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æŠœãå‡ºã•ã‚Œã¾ã™ã€‚
# 3.  **å‡ºåŠ› (`logits`)**: å‡ºåŠ›ã•ã‚Œã‚‹ `logits` ã®shapeã¯ `(B, T, vocab_size)` ã¨ãªã‚Šã¾ã™ã€‚ã“ã‚Œã¯ã€å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å„ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã—ã¦ã€ã€Œæ¬¡ã«æ¥ã‚‹å˜èªžã®å€™è£œï¼ˆå…¨`vocab_size`å€‹ï¼‰ã€ãã‚Œãžã‚Œã®äºˆæ¸¬ã‚¹ã‚³ã‚¢ã‚’æŒã£ã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚
# 
# 
# 
# #### ### ãªãœã“ã‚ŒãŒBigramãƒ¢ãƒ‡ãƒ«ãªã®ã‹ï¼Ÿ
# 
# **Bigramãƒ¢ãƒ‡ãƒ«**ã¯ã€**ç›´å‰ã®1ã¤ã®å˜èªžã ã‘ã‚’è¦‹ã¦æ¬¡ã®å˜èªžã‚’äºˆæ¸¬**ã—ã¾ã™ã€‚
# ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ã¾ã•ã«ãã®è€ƒãˆæ–¹ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚
# 
# * ã‚ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆä¾‹: "hello"ï¼‰ãŒå…¥åŠ›ã•ã‚Œã‚‹ã¨ã€`nn.Embedding`ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ "hello" ã«å¯¾å¿œã™ã‚‹è¡Œã‚’å¼•ã„ã¦ãã¾ã™ã€‚
# * ãã®è¡Œãƒ™ã‚¯ãƒˆãƒ«ï¼ˆã‚µã‚¤ã‚º `vocab_size`ï¼‰ãŒã€æ¬¡ã« "world" ãŒæ¥ã‚‹ç¢ºçŽ‡ã€"there" ãŒæ¥ã‚‹ç¢ºçŽ‡ã€"!" ãŒæ¥ã‚‹ç¢ºçŽ‡...ã¨ã„ã£ãŸã€èªžå½™ã®å…¨å˜èªžã«å¯¾ã™ã‚‹äºˆæ¸¬ã‚¹ã‚³ã‚¢ï¼ˆãƒ­ã‚¸ãƒƒãƒˆï¼‰ãã®ã‚‚ã®ã«ãªã‚‹ã‚ã‘ã§ã™ã€‚
# 
# ã¤ã¾ã‚Šã€ã“ã® `nn.Embedding` ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¯ã€å˜ã«å˜èªžã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹ã ã‘ã§ãªãã€**ã€Œã‚ã‚‹å˜èªžã€ã‹ã‚‰ã€Œæ¬¡ã®å˜èªžã®äºˆæ¸¬ãƒ­ã‚¸ãƒƒãƒˆã€ã¸ã®ãƒžãƒƒãƒ”ãƒ³ã‚°**ã¨ã„ã†ã€ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬æ©Ÿèƒ½ãã®ã‚‚ã®ã‚’æ‹…ã£ã¦ã„ã¾ã™ã€‚ã“ã‚ŒãŒã“ã®ãƒ¢ãƒ‡ãƒ«ã®æ ¸å¿ƒéƒ¨åˆ†ã§ã™ã€‚ðŸ‘

# %%


import torch
import torch.nn as nn
from torch.nn import functional as F
torch.manual_seed(1337)

class BigramLanguageModel(nn.Module):
    def __init__(self, vocab_size:int) -> None:
        super().__init__()
        # each token directory reads of the logits for the net token from a lookup table
        self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=vocab_size)

    def forward(self, idx, targets:torch.Tensor|None=None):
        # idx and targets are both (B, T) tensor of integers
        logits = self.token_embedding_table(idx)
        if target is None:
            loss = None
        elif targets is not None:
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)
        else:
            loss = None

        return logits, loss

    def generate(self, idx, max_new_token):
        # idx is (B, T) array of indicies in the current context
        for _ in range(max_new_token):
            # get the predictions
            logits, loss = self(idx)
            # focus only on the last time step
            logits = logits[:, -1, :] # become (B, C)
            # apply softmax to get probabilities
            probs = F.softmax(logits, dim=-1) # (B, C)
            # sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)
            # append sampled index to the running sequence
            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)
        return idx



m = BigramLanguageModel(vocab_size)
logits, loss = m(xb, yb)
print(logits.shape)
print(loss)

print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_token=100)[0].tolist()))


# %%


# create a PyTorch optimizer
optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)


# %%


batch_size = 32
for steps in range(10000):

    # sample a batch of data
    xb, yb, = get_batch('train')

    # evaluate the loss
    logits, loss = m(xb, yb)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()

    print(loss.item())


# %%


print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_token=500)[0].tolist()))


# # The mathematical trick in self-attention

# %%


torch.manual_seed(1337)
B, T, C = 4, 8, 2
x = torch.randn(B, T, C)
x.shape


# %%


# We want x[b, t] = mean_{i<=t} x[b, i]
# version 1
xbow = torch.zeros((B, T, C)) # x bag_of_words
for b in range(B):
    for t in range(T):
        xprev = x[b, :t+1] # (t, C)
        xbow[b, t] = torch.mean(xprev, 0)


# %%


# version 2
wei = torch.tril(torch.ones((T,T)))
wei = wei / wei.sum(1, keepdim=True)
xbow2 = wei @ x     # (B, T, T) @ (B, T, C) ---> (B, T, C)

# check xbow == xbow2
torch.max(xbow - xbow2)


# %%


# version3
tril = torch.tril(torch.ones(T, T))
wei = torch.zeros((T, T))
wei = wei.masked_fill(tril == 0, float('-inf'))
wei = F.softmax(wei, dim=-1)
xbow3 = wei @ x
torch.max(xbow - xbow3)


# In[ ]:


torch.manual_seed(42)
a = torch.tril(torch.ones(3,3))
a = a / torch.sum(a, 1, keepdim=True)
b = torch.randint(0, 10, (3,2)).float()
c = a @ b

print('a=')
print(a)
print('--')
print('b=')
print(b)
print('--')
print('c=')
print(c)


# In[ ]:




